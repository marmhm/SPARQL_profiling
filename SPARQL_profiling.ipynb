{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51abc28",
   "metadata": {},
   "source": [
    "## Parse sparql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4d009-1c50-48ab-aed1-5320345c6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parse queries\n",
    "# Install Node.js dependencies\n",
    "!npm install sparqljs csv-parser csv-stringify\n",
    "\n",
    "# Create the JavaScript file\n",
    "js_code = \"\"\"\n",
    "const fs = require('fs');\n",
    "const SparqlParser = require('sparqljs').Parser;\n",
    "const csvParser = require('csv-parser');\n",
    "const { stringify } = require('csv-stringify');\n",
    "\n",
    "const parser = new SparqlParser();\n",
    "\n",
    "async function executeQuery(query) {\n",
    "  try {\n",
    "    const parsedQuery = parser.parse(query);\n",
    "    return JSON.stringify(parsedQuery);\n",
    "  } catch (error) {\n",
    "    console.error('Error parsing query:', query);\n",
    "    return 'Error parsing the query.';\n",
    "  }\n",
    "}\n",
    "\n",
    "async function writeBatchToCsv(batch, outputFile) {\n",
    "  return new Promise((resolve, reject) => {\n",
    "    const writeStream = fs.createWriteStream(outputFile, { flags: 'a' });\n",
    "    const csvStringifier = stringify({ header: false, columns: [ 'Parsed_Query','Count'], delimiter: ',' });\n",
    "\n",
    "    writeStream.on('error', (error) => {\n",
    "      reject(error);\n",
    "    });\n",
    "\n",
    "    csvStringifier.pipe(writeStream);\n",
    "\n",
    "    csvStringifier.on('end', () => {\n",
    "      writeStream.end();\n",
    "      resolve();\n",
    "    });\n",
    "\n",
    "    batch.forEach((entry) => {\n",
    "      csvStringifier.write([ entry.Parsed_Query , entry.Count ]);\n",
    "    });\n",
    "\n",
    "    csvStringifier.end();\n",
    "  });\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "  const inputCsvFile = 'query_prefixes_added.csv';\n",
    "  const outputCsvFile = 'wikidata-robotic-parsed.csv';\n",
    "  const batchSize = 1000000;\n",
    "\n",
    "  const readStream = fs.createReadStream(inputCsvFile).pipe(csvParser());\n",
    "\n",
    "  let batch = [];\n",
    "  for await (const row of readStream) {\n",
    "    const sparqlQuery = row['query'];\n",
    "    const count = row['count'];\n",
    "\n",
    "    const parsedQuery = await executeQuery(sparqlQuery);\n",
    "\n",
    "    batch.push({ Parsed_Query: parsedQuery ,Count: count });\n",
    "\n",
    "    if (batch.length >= batchSize) {\n",
    "      await writeBatchToCsv(batch, outputCsvFile);\n",
    "      console.log(`Processed ${batch.length} queries.`);\n",
    "      batch = [];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if (batch.length > 0) {\n",
    "    await writeBatchToCsv(batch, outputCsvFile);\n",
    "  }\n",
    "\n",
    "  console.log('All SPARQL queries executed and results written to output CSV file.');\n",
    "}\n",
    "\n",
    "main();\n",
    "\"\"\"\n",
    "\n",
    "# Write JavaScript code to a file\n",
    "with open(\"optimized.js\", \"w\") as f:\n",
    "    f.write(js_code)\n",
    "\n",
    "# Python code to run the JavaScript script\n",
    "python_code = \"\"\"\n",
    "import subprocess\n",
    "\n",
    "def run_script():\n",
    "    # Run the JavaScript code using Node.js with an increased memory limit\n",
    "    result = subprocess.run(['node', '--max-old-space-size=30720', 'optimized.js'], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output (JSON representation of the parsed SPARQL query)\n",
    "    print(result.stdout)\n",
    "    print('hi')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_script()\n",
    "\"\"\"\n",
    "\n",
    "# Write Python code to a file\n",
    "with open(\"optimized_from_js_.py\", \"w\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "# Execute the script directly\n",
    "print(\"Running the script...\")\n",
    "\n",
    "!python optimized_from_js_.py > parseoutput.txt 2>&1\n",
    "\n",
    "# Print the log output\n",
    "with open(\"parseoutput.txt\", \"r\") as log_file:\n",
    "    output = log_file.read()\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0c86d",
   "metadata": {},
   "source": [
    "##  We count triple patterns of parsed queries, to calculate complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9905e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_subject_complexity(parsed_query):\n",
    "    return parsed_query.count(\"subject\")\n",
    "\n",
    "# Replace 'input.csv' with the name of your input CSV file and 'output.csv' with the desired output CSV file name\n",
    "input_file = 'parsed-output.csv'\n",
    "output_file = 'parsed-output_complexity.csv\n",
    "\n",
    "# Read the CSV file into a DataFrame with appropriate column names\n",
    "column_names = ['query', 'parsed query']  # Adjust these column names as needed\n",
    "df = pd.read_csv(input_file, header=None, names=column_names)\n",
    "\n",
    "# Calculate complexity for each row and create a new column 'complexity'\n",
    "df['complexity'] = df['parsed query'].apply(count_subject_complexity)\n",
    "\n",
    "# Create a new DataFrame with 'query', 'parsed query', and 'complexity' columns\n",
    "result_df = df[['query', 'parsed query', 'complexity']]\n",
    "\n",
    "result_df = result_df.sort_values(by='complexity', ascending=False)\n",
    "\n",
    "# Save the result DataFrame to a new CSV file with headers\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print('sth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f30f92",
   "metadata": {},
   "source": [
    "## We calculate Informativness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab777805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to count variables and named nodes in the bgp section\n",
    "def count_in_bgp(bgp_data):\n",
    "    variable_count = 0\n",
    "    named_node_count = 0\n",
    "\n",
    "    if \"triples\" in bgp_data:\n",
    "        triples = bgp_data[\"triples\"]\n",
    "        for triple in triples:\n",
    "            subject_term_type = triple[\"subject\"].get(\"termType\", None)\n",
    "            predicate_term_type = triple[\"predicate\"].get(\"termType\", None)\n",
    "            object_term_type = triple[\"object\"].get(\"termType\", None)\n",
    "\n",
    "            if subject_term_type == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif subject_term_type == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "\n",
    "            if predicate_term_type == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif predicate_term_type == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "\n",
    "            if object_term_type == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif object_term_type == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "\n",
    "    return variable_count, named_node_count\n",
    "\n",
    "# Function to calculate informativeness and update CSV rows\n",
    "def calculate_informativeness(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + ['Informativeness']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            parsed_query = row['Parsed_Query']\n",
    "\n",
    "            try:\n",
    "                data = json.loads(parsed_query)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON in row: {row}\")\n",
    "                continue\n",
    "\n",
    "            total_variable_count = 0\n",
    "            total_named_node_count = 0\n",
    "\n",
    "            # Traverse through all \"bgp\" sections and count variables and named nodes in bgps\n",
    "            def traverse(data):\n",
    "                nonlocal total_variable_count, total_named_node_count\n",
    "                if isinstance(data, dict):\n",
    "                    if \"type\" in data and data[\"type\"] == \"bgp\":\n",
    "                        variable_count, named_node_count = count_in_bgp(data)\n",
    "                        total_variable_count += variable_count\n",
    "                        total_named_node_count += named_node_count\n",
    "                    for key, value in data.items():\n",
    "                        traverse(value)\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        traverse(item)\n",
    "\n",
    "            # Start traversing from the main JSON dictionary\n",
    "            traverse(data)\n",
    "\n",
    "            # Update the row with informativeness values\n",
    "            if total_variable_count + total_named_node_count == 0:\n",
    "                informativeness = 0.0  # Default value when denominator is zero\n",
    "            else:\n",
    "                informativeness = total_named_node_count / (total_variable_count + total_named_node_count)\n",
    "\n",
    "            row['Informativeness'] = f'{informativeness}'\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Provide the input and output file paths\n",
    "input_csv_file = 'parsed-output.csv'\n",
    "output_csv_file = 'informativness.csv'\n",
    "\n",
    "# Call the function to calculate informativeness and update the output CSV file\n",
    "calculate_informativeness(input_csv_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36ee5c",
   "metadata": {},
   "source": [
    "## Find queries containing FILTER regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c6b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def queries_containing_FILTER(Query):\n",
    "    return Query.count(\"FILTER regex\")\n",
    "\n",
    "# Replace 'input.csv' with the name of your input CSV file and 'output.csv' with the desired output CSV file name\n",
    "input_file = 'query.csv'\n",
    "output_file = 'query-with_filter.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "df['filterReg'] = df['Query'].apply(queries_containing_FILTER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837b2b2",
   "metadata": {},
   "source": [
    "## Find queries containing OPTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12719d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_optional(Query):\n",
    "    return Query.count(\"OPTIONAL\")\n",
    "\n",
    "# Replace 'input.csv' with the name of your input CSV file and 'output.csv' with the desired output CSV file name\n",
    "input_file = 'query.csv'\n",
    "output_file = 'query-OPTIONAL.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create a new column 'complexity' by counting the occurrences of 'subject' in 'parsed_query'\n",
    "df['OPTIONAL'] = df['Query'].apply(count_optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c6de17",
   "metadata": {},
   "source": [
    "## Find the queries with multiple prefixes(federation or high Interlinkness)\n",
    "\n",
    "list of target prefixes:\n",
    "\n",
    "DrugBank Vocabulary: http://bio2rdf.org/drugbank_vocabulary\n",
    "\n",
    "PharmGKB Vocabulary: http://bio2rdf.org/pharmgkb_vocabulary\n",
    "\n",
    "ClinicalTrials Vocabulary: http://bio2rdf.org/clinicaltrials_vocabulary\n",
    "\n",
    "OMIM Vocabulary: http://bio2rdf.org/omim_vocabulary\n",
    "\n",
    "MeSH Vocabulary: http://bio2rdf.org/mesh_vocabulary\n",
    "\n",
    "Uniprot Vocabulary: http://bio2rdf.org/uniprot_vocabulary\n",
    "\n",
    "Gene Ontology Vocabulary: http://bio2rdf.org/go_vocabulary\n",
    "\n",
    "Reactome Vocabulary: http://bio2rdf.org/reactome_vocabulary\n",
    "\n",
    "NCBI Taxonomy Vocabulary: http://bio2rdf.org/taxonomy_vocabulary\n",
    "\n",
    "PubChem Vocabulary: http://bio2rdf.org/pubchem_vocabulary\n",
    "\n",
    "PDB Vocabulary: http://bio2rdf.org/pdb_vocabulary\n",
    "\n",
    "InterPro Vocabulary: http://bio2rdf.org/interpro_vocabulary\n",
    "\n",
    "KEGG Vocabulary: http://bio2rdf.org/kegg_vocabulary\n",
    "\n",
    "CHEBI Vocabulary: http://bio2rdf.org/chebi_vocabulary\n",
    "\n",
    "\n",
    "\n",
    "['http://bio2rdf.org/pharmgkb_vocabulary:', 'http://bio2rdf.org/genatlas_vocabulary:', 'http://bio2rdf.org/chebi_vocabulary:', 'http://bio2rdf.org/chemspider_vocabulary:', 'http://bio2rdf.org/clinicaltrials_vocabulary:', 'http://bio2rdf.org/genbank_vocabulary:', 'http://bio2rdf.org/snomedct_vocabulary:', 'http://bio2rdf.org/ctd_vocabulary:', 'http://bio2rdf.org/dailymed_vocabulary:', 'http://bio2rdf.org/iuphar.ligand_vocabulary:', 'http://bio2rdf.org/ensembl_vocabulary:', 'http://bio2rdf.org/go_vocabulary:', 'http://bio2rdf.org/dbsnp_vocabulary:', 'http://bio2rdf.org/pubchem.substance_vocabulary:', 'http://bio2rdf.org/refseq_vocabulary:', 'http://bio2rdf.org/pubchem.compound_vocabulary:', 'http://bio2rdf.org/pdb_vocabulary:', 'http://bio2rdf.org/bindingdb_vocabulary:', 'http://bio2rdf.org/pubmed_vocabulary:', 'http://bio2rdf.org/huge_vocabulary:', 'http://bio2rdf.org/ccd_vocabulary:', 'http://bio2rdf.org/humancyc_vocabulary:', 'http://bio2rdf.org/atc_vocabulary:', 'http://bio2rdf.org/mesh_vocabulary:', 'http://bio2rdf.org/hgnc_vocabulary:', 'http://bio2rdf.org/ttd_vocabulary:', 'http://bio2rdf.org/ncbigene_vocabulary:', 'http://bio2rdf.org/uniprot_vocabulary:', 'http://bio2rdf.org/modbase_vocabulary:', 'http://bio2rdf.org/umls_vocabulary:', 'http://bio2rdf.org/drugbank_vocabulary:', 'http://bio2rdf.org/dpd_vocabulary:', 'http://bio2rdf.org/omim_vocabulary:', 'http://bio2rdf.org/kegg_vocabulary:', 'http://bio2rdf.org/mutdb_vocabulary:', 'http://bio2rdf.org/ndc_vocabulary:', 'http://bio2rdf.org/ndfrt_vocabulary:', 'http://bio2rdf.org/iuphar.receptor_vocabulary:', 'http://bio2rdf.org/hgnc.symbol_vocabulary:', 'http://bio2rdf.org/alfred_vocabulary:', 'http://bio2rdf.org/meddra_vocabulary:', 'http://bio2rdf.org/genecards_vocabulary:' ,'http://bio2rdf.org/interpro_vocabulary:', 'http://bio2rdf.org/reactome_vocabulary:', 'http://bio2rdf.org/mgi_vocabulary:']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e099a",
   "metadata": {},
   "source": [
    "## Interlinkness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "input_file = 'query.csv'\n",
    "output_file = 'query-interlinkness.csv'\n",
    "\n",
    "keyword_list = ['http://bio2rdf.org/pharmgkb_vocabulary:', 'http://bio2rdf.org/genatlas_vocabulary:', 'http://bio2rdf.org/chebi_vocabulary:', 'http://bio2rdf.org/chemspider_vocabulary:', 'http://bio2rdf.org/clinicaltrials_vocabulary:', 'http://bio2rdf.org/genbank_vocabulary:', 'http://bio2rdf.org/snomedct_vocabulary:', 'http://bio2rdf.org/ctd_vocabulary:', 'http://bio2rdf.org/dailymed_vocabulary:', 'http://bio2rdf.org/iuphar.ligand_vocabulary:', 'http://bio2rdf.org/ensembl_vocabulary:', 'http://bio2rdf.org/go_vocabulary:', 'http://bio2rdf.org/dbsnp_vocabulary:', 'http://bio2rdf.org/pubchem.substance_vocabulary:', 'http://bio2rdf.org/refseq_vocabulary:', 'http://bio2rdf.org/pubchem.compound_vocabulary:', 'http://bio2rdf.org/pdb_vocabulary:', 'http://bio2rdf.org/bindingdb_vocabulary:', 'http://bio2rdf.org/pubmed_vocabulary:', 'http://bio2rdf.org/huge_vocabulary:', 'http://bio2rdf.org/ccd_vocabulary:', 'http://bio2rdf.org/humancyc_vocabulary:', 'http://bio2rdf.org/atc_vocabulary:', 'http://bio2rdf.org/mesh_vocabulary:', 'http://bio2rdf.org/hgnc_vocabulary:', 'http://bio2rdf.org/ttd_vocabulary:', 'http://bio2rdf.org/ncbigene_vocabulary:', 'http://bio2rdf.org/uniprot_vocabulary:', 'http://bio2rdf.org/modbase_vocabulary:', 'http://bio2rdf.org/umls_vocabulary:', 'http://bio2rdf.org/drugbank_vocabulary:', 'http://bio2rdf.org/dpd_vocabulary:', 'http://bio2rdf.org/omim_vocabulary:', 'http://bio2rdf.org/kegg_vocabulary:', 'http://bio2rdf.org/mutdb_vocabulary:', 'http://bio2rdf.org/ndc_vocabulary:', 'http://bio2rdf.org/ndfrt_vocabulary:', 'http://bio2rdf.org/iuphar.receptor_vocabulary:', 'http://bio2rdf.org/hgnc.symbol_vocabulary:', 'http://bio2rdf.org/alfred_vocabulary:', 'http://bio2rdf.org/meddra_vocabulary:', 'http://bio2rdf.org/genecards_vocabulary:' ,'http://bio2rdf.org/interpro_vocabulary:', 'http://bio2rdf.org/reactome_vocabulary:', 'http://bio2rdf.org/mgi_vocabulary:']\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to count the number of keywords in a string\n",
    "def count_keywords(query, keywords):\n",
    "    return sum(1 for keyword in keywords if keyword in query)\n",
    "\n",
    "# Apply the function to each row and create the 'Interlinkness' column\n",
    "df['Interlinkness'] = df['Query'].apply(lambda x: count_keywords(x, keyword_list))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
